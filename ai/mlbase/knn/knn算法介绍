
1.knn:k近邻算法.每个样本都可以用它最接近的k个邻居表示

2.回归和分类中的knn算法:
    knn算法在回归和分类应用中是有所区别的,区别在于决策方式的不同.
在分类预测时,一般采用多数表决法;
在回归预测时,一般采用平均值法或者加权平均值法

3.算法原理:
    第一,从训练样本中获取k个距离预测样本最近的样本数据
    第二,根据获取的k个样本预测样本的目标属性值

4.算法缺陷:
    k值选取过小时,模型越复杂,则可能会出现过拟合现象.此时可以增加样本数
    k值选取适当时,但是得到的近邻数目类别一致,无法预测样本目标属性,这是欠拟合现象

5.距离的度量:
    一般使用欧式距离

6.k值选择：
    交叉验证
    网格法

7.knn算法难点在于找到k个最邻点,一般有以下两种方式:
    蛮力实现:计算预测样本到所有训练集样本的距离,然后选择最小的k个.当特征数较多,
样本数较多时,算法执行效率较低(样本数量较少时可以选择此方式)
    kd-tree:首先对训练数据进行建模,并构建kd树,然后再根据构建好的模型来获取邻近
样本数据

8.kd树构建方式:从m个样本的n维特征中,分别计算n个特征值的方差,用方差最大的第k维特征
作为根节点.对于该列特征,选择中值作为划分点,小于划分点值的为左子树,大于或等于的为右
子树.然后依次递归构建kd树.


方差越大,区分能力越强
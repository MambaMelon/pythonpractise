
1.高信息熵:表示随机变量x是均匀分布的
2.低信息熵:表示随机变量x各种取值不是等概率出现

3.条件熵:在给定条件x的前提下,随机变量y的信息熵就叫做条件熵H(Y|X)

4.决策树:有监督分类算法
    在已知各种情况发生概率的基础上,通过构建决策树来进行分析的一种的方式
是一种直观应用概率分析的一种图解法.
    决策树内部每个节点表示属性的测试,每个分支表示一个测试输出,每个叶节点
代表一种类别.

5.决策树分为两类:分类树和回归树.前者用于分类标签值,后者用于预测连续值.

6.决策树构建:关键步骤在于分裂属性.是指在某个节点按照某一类特征属性的不同划分构建不同
分支,最终目的是让各个分裂子集纯度高.纯:让一个分裂子类中的待分类的项尽可能的属于同一个级别.
    1)将所有的特征看成一个一个的节点;
    2)遍历每个特征的每一种分割方式,找到最好的分割点;将数据划分为不同的子节点，eg：N1、
        N2....Nm;计算划分之后所有子节点的'纯度'信息;
    3)对第二步产生的分割,选择出最优的特征以及最优的划分方式;得出最终的子节点:N1、N2....Nm
    4)对子节点N1、N2....Nm分别继续执行2-3步,直到每个最终的子节点都足够'纯'.

7.决策树量化纯度:Gini系数,熵,错误率.三个值越大,表示数据纯度越低.
8.信息增益度:
    当计算出各个特征的量化纯度值后,使用信息增益度来选择出当前数据集的分割特征属性.如果信息增益度
越大,表示在该特征属性上会损失的纯度越大,那么该属性就应该在决策树的上层.计算公式为:
    H(D)-H(D|A)
9.决策树的停止条件:
    1)每个节点只有一种类型的时候停止构建
    2)当前节点中记录数小于某个阈值,同时迭代次数达到给定值时
10.决策树效果评估
    采用混淆矩阵计算准确率等指标
    计算叶子节点的纯度值总和

11.ID3算法:每次迭代选择信息增益最大的特征属性作为分割属性
    该算法只支持离线的特征属性
    该算法构建的是多叉树
    优点:
        构建快速简单
    缺点:
        依赖于特征数目较多的特征
        不是递增算法
        单变量决策树,不会考虑特征之间的关联
        抗燥性差
        需要将数据加载到内存

12.C4.5算法:优化的ID3算法,使用信息增益率替代信息增益.在树的构造过程中会进行剪枝等优化操作.
    还能够完成连续数据的离散化处理.

13.CART算法:使用基尼系数作为数据纯度的量化指标来构建的决策树算法就叫做CART
    (Classification And Regression Tree，分类回归树)算法

14.算法优化
    剪枝操作:前置剪枝、后置剪枝
    随机森林
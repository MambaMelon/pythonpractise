2018-7-3

1.概念：将若干个学习器组合生成一个新的学习器，确保了弱分类器的多样性，
    同时保证了算法的性能。

2.为什么需要集成学习?
    1)合并弱分类器，边界趋于合理
    2)过大或过小数据集，使用数据子集训练不同分类器后合并
    3)边界过于复杂的数据集，可以训练多个模型融合
    4)异构的特征数据集无法融合，所以为每个数据集构建一个分类模型并融合

3.当我们对Adaboost调参时，主要要对两部分内容进行调参:
    第一部分是对我们的Adaboost的框架进行调参
    第二部分是对我们选择的弱分类器进行调参。两者相辅相成

4.GBDT优缺点:
    优点：
        可以处理连续值和离散值
        参数调整相对较少次，但效果不差
        鲁棒性比较强
    缺点:
        弱学习器之间的存在关联关系，难以并行训练模型

5.bagging和boosting的比较：
    1)样本选择：Bagging算法是有放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例在
    分类器中的权重发生变化或者目标属性y发生变化，而权重&y值都是根据上一轮的分类结果进行调整；
    2)样例权重：Bagging使用随机抽样，样例是等权重；Boosting根据错误率不断的调整样例的权重值，错误率越大
    则权重越大(Adaboost)；
    3)预测函数：Bagging所有预测模型的权重相等；Boosting算法对于误差小的分类器具有更大的权重（Adaboost）。
    4)并行计算：Bagging算法可以并行生成各个基模型；Boosting理论上只能顺序生产，因为后一个模型需要前一个
    模型的结果；
    5)Bagging是减少模型的variance(方差)；Boosting是减少模型的Bias(偏度)。
    6)Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合；Boosting里每个分类
    模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合

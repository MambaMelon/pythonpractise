2018-7-3

1.概念：将若干个学习器组合生成一个新的学习器，确保了弱分类器的多样性，
    同时保证了算法的性能。

2.为什么需要集成学习?
    1)合并弱分类器，边界趋于合理
    2)过大或过小数据集，使用数据子集训练不同分类器后合并
    3)边界过于复杂的数据集，可以训练多个模型融合
    4)异构的特征数据集无法融合，所以为每个数据集构建一个分类模型并融合

3.当我们对Adaboost调参时，主要要对两部分内容进行调参:
    第一部分是对我们的Adaboost的框架进行调参
    第二部分是对我们选择的弱分类器进行调参。两者相辅相成

4.GBDT优缺点:
    优点：
        可以处理连续值和离散值
        参数调整相对较少次，但效果不差
        鲁棒性比较强
    缺点:
        弱学习器之间的存在关联关系，难以并行训练模型

5.bagging和boosting的比较：
    1)样本选择：Bagging算法是有放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例在
    分类器中的权重发生变化或者目标属性y发生变化，而权重&y值都是根据上一轮的分类结果进行调整；
    2)样例权重：Bagging使用随机抽样，样例是等权重；Boosting根据错误率不断的调整样例的权重值，错误率越大
    则权重越大(Adaboost)；
    3)预测函数：Bagging所有预测模型的权重相等；Boosting算法对于误差小的分类器具有更大的权重（Adaboost）。
    4)并行计算：Bagging算法可以并行生成各个基模型；Boosting理论上只能顺序生产，因为后一个模型需要前一个
    模型的结果；
    5)Bagging是减少模型的variance(方差)；Boosting是减少模型的Bias(偏度)。
    6)Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合；Boosting里每个分类
    模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合

6.XGBoost学习策略:
    当树的结构确定的时候，我们可以得到最优的叶子点分数以及对应的最小损失值，问题在于如何确定树结构？
暴力穷举所有可能的结构，选择损失值最小的(很难求解);贪心法，每次尝试选择一个分裂点进行分裂，计算操作
前后的增益，选择增益最大的方式进行分裂。
    决策树相关算法计算指标：
ID3算法：信息增益
C4.5算法：信息增益率
CART算法：Gini系数

7.XGBoost的其它特性：
    列采样(column subsampling)：借鉴随机森林的做法，支持列抽样，不仅可以降低过拟合，还可以减少计算量；
    支持对缺失值的自动处理。对于特征的值有缺失的样本，XGBoost可以自动学习分裂方向；
    XGBoost支持并行。XGBoost的并行是特征粒度上的，在计算特征的Gain的时候，会并行执行，但是在树的构建过程中，还是串行构建的；
    XGBoost算法中加入正则项，用于控制模型的复杂度，最终模型更加不容易过拟合；
    XGBoost基学习器支持CART、线性回归、逻辑回归；
    XGBoost支持自定义损失函数(要求损失函数二阶可导)

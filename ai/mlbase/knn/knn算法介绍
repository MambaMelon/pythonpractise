
1.knn:k近邻算法.每个样本都可以用它最接近的k个邻居表示

2.回归和分类中的knn算法:
    knn算法在回归和分类应用中是有所区别的,区别在于决策方式的不同.
在分类预测时,一般采用多数表决法;
在回归预测时,一般采用平均值法或者加权平均值法

3.算法原理:
    第一,从训练样本中获取k个距离预测样本最近的样本数据
    第二,根据获取的k个样本预测样本的目标属性值

4.算法缺陷:
    k值选取过小时,模型越复杂,则可能会出现过拟合现象.此时可以增加样本数
    k值选取适当时,但是得到的近邻数目类别一致,无法预测样本目标属性,这是欠拟合现象

5.距离的度量:
    一般使用欧式距离

6.k值选择：
    交叉验证
    网格法

7.knn算法难点在于找到k个最邻点,一般有以下两种方式:
    蛮力实现:计算预测样本到所有训练集样本的距离,然后选择最小的k个.当特征数较多,
样本数较多时,算法执行效率较低(样本数量较少时可以选择此方式)
    kd-tree:首先对训练数据进行建模,并构建kd树,然后再根据构建好的模型来获取邻近
样本数据

8.kd树构建方式:从m个样本的n维特征中,分别计算n个特征值的方差,用方差最大的第k维特征
作为根节点.对于该列特征,选择中值作为划分点,小于划分点值的为左子树,大于或等于的为右
子树.然后依次递归构建kd树.

9.kd树查找最近邻:
    找到所属叶子节点,以目标点为圆心,以目标点到叶子节点的距离为半径画圆体;
    如果圆体与分割线有交线,那么考虑另一个叶子结点,比较距离并更新目标节点;
    重复向上遍历即可得到最终的最近邻点.

10.knn参数:
    weights:uniform-等权重;distance-权重和距离成反比
    n_neighbors:邻近数目,默认为5
    algorithm:auto/ball_tree/kd_tree/brute
    leaf_size:kd_tree时的叶子数量,默认为50
    metric:闵可夫斯基距离度量公式
    p:



方差越大,区分能力越强
--基于暴力形式的knn实现
